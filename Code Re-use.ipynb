{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print all Performance metrics:\n",
    "\n",
    "#### Initialize the data frames to store and compare the performance metrics of the models.\n",
    "\n",
    "I have this below code snippet to store the model performance scores in a data frame to compare the different models after the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning using Grid search :\n",
    "#### Function to obtain the best model by performing hyperparameter tuning using GridSearchCV .\n",
    "\n",
    "I have defined a function “get_best_hyperparameters” which does the hyperparameter tuning using GridSearchCV by taking classifier or regressor model as input. This function returns the best model which can be used to fit and predict. This step can be skipped if one just wants to build a basic model without performing any hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For both Classifier and Regressor\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "def get_best_hyperparameters(model, params, cv_value , X_train, y_train ): \n",
    "    search = GridSearchCV(estimator=model, param_grid=params, n_jobs=-1, verbose=1,cv=cv_value) \n",
    "    search.fit(X_train, y_train)  \n",
    "    print(\"Best Accuracy    :\",  search.best_score_) \n",
    "    print(\"Best Parameters  : \", search.best_params_)\n",
    "    print(\"Best Estimators : \",  search.best_estimator_)  \n",
    "    best_grid = search.best_estimator_\n",
    "    return best_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fit and Predict:\n",
    "#### Function to fit and predict the model:\n",
    "\n",
    "This function (for classifier and regressor) get_classifier_predictions / get_regressor_predictions takes in the model as input and returns the predicted train and test results. In case of classifier , it also returns predicted train and test probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Classifier\n",
    "def get_classifier_predictions(classifier, X_train, y_train, X_test): \n",
    "    classifier.fit(X_train,y_train)\n",
    "    y_pred_train =classifier.predict(X_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "    y_pred_prob_train = classifier.predict_proba(X_train)\n",
    "    y_pred_prob_test = classifier.predict_proba(X_test)\n",
    "    return y_pred_train, y_pred_test, y_pred_prob_train,y_pred_prob_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Regressor\n",
    "def get_regressor_predictions(regressor, X_train, y_train, X_test):  \n",
    "    regressor.fit(X_train,y_train)\n",
    "    y_pred_train =regressor.predict(X_train)\n",
    "    y_pred_test = regressor.predict(X_test)\n",
    "    return y_pred_train, y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics:\n",
    "#### Function to calculate and print the performance metrics of train and test dataset\n",
    "\n",
    "The function print_classifier_scores / print_regressor_scores calculates and returns the dataset with all the performance metrics scores related to a classification / regression algorithm respectively ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Classifier\n",
    "from sklearn.metrics import accuracy_score ,confusion_matrix ,precision_score , recall_score , f1_score, plot_confusion_matrix ,roc_auc_score\n",
    "import matplotlib.pyplot as plt                                     # Importing pyplot interface to use matplotlib\n",
    "%matplotlib inline\n",
    "def print_classifier_scores(classifier, X_train, X_test, y_train ,y_test,y_pred_train, y_pred_test,y_pred_prob_train, y_pred_prob_test,algorithm):\n",
    "# store classifier scores for Training Dataset\n",
    "    v_recall_score_train =  recall_score(y_train,y_pred_train)\n",
    "    v_precision_score_train = precision_score(y_train,y_pred_train)\n",
    "    v_f1_score_train =  f1_score(y_train,y_pred_train)\n",
    "    v_accuracy_score_train = accuracy_score(y_train,y_pred_train)\n",
    "    v_roc_auc_train = roc_auc_score(y_train, y_pred_prob_train[:,1])\n",
    "    \n",
    "# print classifier scores for Training Dataset\n",
    "    print('Train-Set Confusion Matrix:\\n', confusion_matrix(y_train,y_pred_train)) \n",
    "    print(\"Recall Score    : \", v_recall_score_train)\n",
    "    print(\"Precision Score : \", v_precision_score_train)\n",
    "    print(\"F1 Score        : \", v_f1_score_train)\n",
    "    print(\"Accuracy Score  : \", v_accuracy_score_train)\n",
    "    print(\"ROC AUC         :  {}\".format(v_roc_auc_train))\n",
    "    print(\"Predict Probability  :\" , y_pred_prob_train)\n",
    "    plot_confusion_matrix(classifier, X_train , y_train , display_labels = [\"1\" , \"0\"])\n",
    "    plt.grid(b=None)\n",
    "# store classifier scores for Testing Dataset \n",
    "   \n",
    "    v_recall_score_test =  recall_score(y_test,y_pred_test)\n",
    "    v_precision_score_test = precision_score(y_test,y_pred_test)\n",
    "    v_f1_score_test =  f1_score(y_test,y_pred_test)\n",
    "    v_accuracy_score_test = accuracy_score(y_test,y_pred_test)\n",
    "    v_roc_auc_test = roc_auc_score(y_test, y_pred_prob_test[:,1])\n",
    "# Print classifier scores for Testing Dataset    \n",
    "    print('Test-Set Confusion Matrix:\\n', confusion_matrix(y_test,y_pred_test)) \n",
    "    print(\"Recall Score    : \", v_recall_score_test)\n",
    "    print(\"Precision Score : \", v_precision_score_test)\n",
    "    print(\"F1 Score        : \", v_f1_score_test)\n",
    "    print(\"Accuracy Score  : \", v_accuracy_score_test)\n",
    "    print(\"ROC AUC         :  {}\".format(v_roc_auc_test))\n",
    "    print(\"Predict Probability  :\" , y_pred_prob_test)\n",
    "    plot_confusion_matrix(classifier, X_test , y_test , display_labels = [\"1\" , \"0\"])\n",
    "    plt.grid(b=None)\n",
    "# store to append the results in dataframe for final comparison of performance \n",
    "    df_model_test_train_acc = dict({'Model' : algorithm, 'Train Accuracy Score' :v_accuracy_score_train,'Test Accuracy Score' :v_accuracy_score_test })\n",
    "    df_model_performance = dict({'Model' : algorithm, 'Accuracy Score' :v_accuracy_score_test, 'F1 Score' : v_f1_score_test, 'Precision Score' : v_precision_score_test, 'Recall Score' :v_recall_score_test, 'ROC AUC' : v_roc_auc_test})\n",
    "    \n",
    "    return df_model_test_train_acc , df_model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For regressor \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "def print_regressor_scores(regressor, X_train, X_test, y_train ,y_test,y_pred_train, y_pred_test,algorithm):\n",
    "    \n",
    "    # store regressor scores for Training Dataset\n",
    "    MAE_train = mean_absolute_error(y_train, y_pred_train)\n",
    "    RMSE_train = np.sqrt( mean_squared_error(y_train, y_pred_train))\n",
    "    r2_score_train = r2_score(y_train, y_pred_train)\n",
    "    # Calculating Adjusted R2 for training set\n",
    "    SS_Residual_train = sum((y_train-y_pred_train)**2)\n",
    "    SS_Total_train = sum((y_train-np.mean(y_train))**2)\n",
    "    r_squared_train = 1 - (float(SS_Residual_train))/SS_Total_train\n",
    "    adj_r_sq_train = 1 - (1-r_squared_train)*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)\n",
    "    \n",
    "    # print regressor scores for Training Dataset\n",
    "    print('MAE for training set is {}'.format(MAE_train))\n",
    "    print('RMSE for training set is {}'.format(RMSE_train))\n",
    "    print('R squared score for training set is {}'.format(r2_score_train))\n",
    "    print('Adjusted R squared score for training set is {}'.format(adj_r_sq_train))\n",
    "    \n",
    "    # store regressor scores for Test Dataset\n",
    "    MAE_test = mean_absolute_error(y_test, y_pred_test)\n",
    "    RMSE_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    r2_score_test = r2_score(y_test, y_pred_test)\n",
    "    # Calculating Adjusted R2 for test set\n",
    "    SS_Residual_test = sum((y_test-y_pred_test)**2)\n",
    "    SS_Total_test = sum((y_test-np.mean(y_test))**2)\n",
    "    r_squared_test = 1 - (float(SS_Residual_test))/SS_Total_test\n",
    "    adj_r_sq_test = 1 - (1-r_squared_test)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n",
    "    \n",
    "    # print regressor scores for Test Dataset \n",
    "    print('MAE for test set is {}'.format(MAE_test))\n",
    "    print('RMSE for test set is {}'.format(RMSE_test))\n",
    "    print('R squared score for test set is {}'.format(r2_score_test))\n",
    "    print('Adjusted R squared score for testing set is {}'.format(adj_r_sq_test))\n",
    "    \n",
    "    # store to append the results in dataframe for final comparison of performance\n",
    "    df_model_test_train_r2= dict({'Model' : algorithm, 'Train Adjusted R2 Score' :adj_r_sq_train,'Test Adjusted R2 Score' :adj_r_sq_test })\n",
    "    df_model_performance = dict({'Model' : algorithm, 'MAE' : MAE_test, 'RMSE' : RMSE_test, 'R2 Score' : r2_score_test, 'Adjusted R2 Score' :adj_r_sq_test})\n",
    "    return df_model_test_train_r2 , df_model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " # For Classifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#This dataframe stores the scores from classifier models\n",
    "df_model=pd.DataFrame(columns=['Model','Accuracy Score' ,'F1 Score', 'Precision Score' , 'Recall Score' ,'ROC AUC'])\n",
    "df_model_performance =df_model\n",
    "#This dataframe stores the train and test accuracy from classifier models to compare at the end of the model building. This can also be further modified to compare the other scores such as F1 score etc\n",
    "df_model_test_train_acc = pd.DataFrame(columns=['Model' , 'Train Accuracy Score' ,'Test Accuracy Score'])\n",
    "df_model_accuracy =df_model_test_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Regressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#This dataframe stores the scores from regressor models\n",
    "df_model=pd.DataFrame(columns=['Model', 'MAE' ,'RMSE', 'R2 Score' , 'Adjusted R2 Score'])\n",
    "df_model_performance =df_model\n",
    "#This data frame stores the train and test \"adjusted R2 scores\" from regressor models to compare at the end of the model building. This can also be further modified to compare the other score such as MSE , RMSE  etc\n",
    "df_model_test_train_r2 = pd.DataFrame(columns=['Model' , 'Train Adjusted R2 Score' ,'Test Adjusted R2 Score'])\n",
    "df_model_r2 =df_model_test_train_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
